import re
import itertools
import sys
import csv
import time
import os
import pandas as pd
import numpy as np
from shutil import copyfile

fileName='data/data.txt'
hashTable={}
bitVector={}
freqItems=[]
freqItemsCurItr=[]
my_dict={}
bitVector=[]
frequent_items=[]             # list of the frequent items
weight=0


def addWeights(d, basket):
    global weight
    for item in basket:
        if item not in d:
            d[item] = weight
            weight += 1

def updateHashTable(line, my_dict, size, max_val):
    global hashTable
    items = [list(x) for x in itertools.combinations(line, size)]

    for pair in items:
        total = pair[0] + pair[1] * max_val
        if total in hashTable:
            hashTable[total] += 1
        else:
            hashTable[total] = 1

def generateFreqCandidates(items):
    global freqItemsCurItr
    temp = []
    freqItemsCurItr = []
    for key in items:
        if items[key] >= support:
            freqItemsCurItr.append(key)
            freqItems.append(key)
    freqItems.sort()
    freqItemsCurItr.sort()
    for tuples in freqItemsCurItr:
        # temp.append(list(tuples))
        temp.append(list(tuples)[0])
    freqItemsCurItr = temp

    return freqItemsCurItr


def countCandidatesAndFillHashTable2(_pass, dataset, max_val):
    global items
    global my_dict  # Has weights for each item in the basket
    global freqItems

    for basket in dataset:

        itemsInBasket = list(itertools.combinations(basket, _pass + 1))

        for item in itemsInBasket:
            if item in items:
                items[item] += 1
            else:
                items[item] = 1
        updateHashTable(basket, my_dict, _pass + 2, max_val)

def generateBitVector(min_val, max_val):
    global bitVector
    bitVector = []

    diff = (max_val - min_val)
    bitVector = np.zeros(diff, dtype=int)

    for val in hashTable:
        if hashTable[val] >= support:
            bitVector[val - min_val] = 1

# Generating items-singletons
if __name__ == '__main__':

    # Testing sets
    chunk_percent = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
    thresholds = [0.01, 0.05, 0.1]

    # basket count
    data_lines = open(fileName).readlines()
    basket_count = len(data_lines)

    # calculate total number of tests
    totalTestCount = len(chunk_percent) * len(thresholds)
    testCount = 0

    print (" # %Thr Supp Chunk  Bucket  Fre      Time")

    # Setup for CSV
    data_result = []

    # Nested loops for test sets
    for threshold in thresholds:
        for percent in chunk_percent:

            # get line count for chunk
            chunk_size = int(basket_count * percent)

            # finding max value for bucket size
            max_val = 0

            # create chunked dataset
            # Note: Max value among all transactions must be found. Will be used in hash fn.
            dataset = []
            with open(fileName) as csvfile:
                reader = csv.reader(csvfile)
                for row in itertools.islice(reader, chunk_size + 1):
                    cells = [ int(i) for i in row[0].split(" ")[:-1] ]
                    dataset.append(cells)
                    if max_val < max(cells):
                        max_val = max(cells)

            # support threshold
            support = int(threshold * chunk_size)

            start = time.time()  # Timer start

            #Pass 1 begins
            _pass = 0
            size = 0
            items = {}
            hashTable = {}

            #Count the candidate pairs and create the hash table
            countCandidatesAndFillHashTable2(_pass, dataset, max_val)

            #Pass 2 begins
            _pass += 1
            frequent_items = []
            bitVector = {}

            #Get list of frequent items from pass 1
            frequent_items = generateFreqCandidates(items)  # list of frequent items
            min_hval = min(int(x) for x in hashTable)       # find the min hash value generated
            max_hval = max(int(x) for x in hashTable)       # find the max hash value generated
            bucketSize = len(hashTable)                     # number of buckets generated in the hashtable
            generateBitVector(min_hval, max_hval)           # create bit vector for next step

            # use the frequent items to create pairs
            current_frequent_pairs = [list(x) for x in itertools.combinations(frequent_items, 2)]

            # Find Frequent Itemsets that have frequent items and pairs that have value 1 in bit vector
            frequent_pairs = []
            # temp_frequent_pairs_print = {}        # use if you need to print the f. item sets for this iteration

            # Note: hashIndex is the key generated by hashing formula to save count of item set
            # Note: hashIndex is the hash(a, b) = a + b * N. Here N is max_val for this iteration
            # Note: hashIndex - min_hval gives the corresponding index of the f. item set in bit vector
            for pair in current_frequent_pairs:
                hashIndex = pair[0] + pair[1] * max_val     # hash elements of pair for hashTable index
                if bitVector[hashIndex - min_hval] == 1:    # if corresponding bit vector bit is 1 then f. pair found
                    frequent_pairs.append(pair)
                    # temp_frequent_pairs_print[tuple(pair)] = hashTable[hashIndex]

            testCount += 1
            end = time.time() # Timer stop

            # Build a list for use in CSV output
            data_result_line = [testCount, int(threshold * 100), support, chunk_size, percent,
                                len(frequent_pairs), (end -start ) *1000]
            data_result.append(data_result_line)

            print ("%2d %4d %4d %5d %7d %4d %9.3f" %
                            (testCount, int(threshold * 100), support, chunk_size, bucketSize, len(frequent_pairs),
                             (end - start) * 1000))

            # Uncomment if printing of frequent pairs is required
            # values = []
            # temp_df = pd.DataFrame( columns=['Keys', 'Support'])
            # for key in temp_frequent_pairs_print:
            #     values = [key, temp_frequent_pairs_print[key]]
            #     temp_df.loc[len(temp_df)] = values
            #
            # temp_df.to_csv('data/pcy_test.csv',index=False)

    df = pd.DataFrame(data_result, columns=["#", "Threshold", "Support", "Chunk Size", "Bucket Size",
                                            "Frequent Set Count", "Run Time"])

    df.to_csv('data/pcy_algo_v2_result.csv', index=False)